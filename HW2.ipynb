{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашка 2. Профилирование пользователей\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Самостоятельно разобраться с тем, что такое tfidf (документация https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html и еще - https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "2. Модифицировать код функции get_user_embedding таким образом, чтобы считалось не среднее (как в примере np.mean), а медиана. Применить такое преобразование к данным, обучить модель прогнозирования оттока и посчитать метрики качества и сохранить их: roc auc, precision/recall/f_score (для 3 последних - подобрать оптимальный порог с помощью precision_recall_curve, как это делалось на уроке)\n",
    "3. Повторить п.2, но используя уже не медиану, а max\n",
    "4. (опциональное, если очень хочется) Воспользовавшись полученными знаниями из п.1, повторить пункт 2, но уже взвешивая новости по tfidf (подсказка: нужно получить веса-коэффициенты для каждого документа. Не все документы одинаково информативны и несут какой-то положительный сигнал). Подсказка 2 - нужен именно idf, как вес.\n",
    "5. Сформировать на выходе единую таблицу, сравнивающую качество 3 разных метода получения эмбедингов пользователей: mean, median, max, idf_mean по метрикам roc_auc, precision, recall, f_score\n",
    "6. Сделать самостоятельные выводы и предположения о том, почему тот или ной способ оказался эффективнее остальных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем необходимые библиотеки\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from razdel import tokenize\n",
    "\n",
    "import pymorphy2\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, classification_report, precision_recall_curve, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Заместитель председателяnправительства РФnСерг...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>Матч 1/16 финала Кубка России по футболу был п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>Форвард «Авангарда» Томаш Заборский прокоммент...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>Главный тренер «Кубани» Юрий Красножан прокомм...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>Решением попечительского совета владивостокско...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4900</td>\n",
       "      <td>И.о. главного тренера Вячеслав Буцаев прокомме...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4901</td>\n",
       "      <td>Запорожский «Металлург» дома потерпел разгромн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4902</td>\n",
       "      <td>Сборная США одержала победу над Австрией со сч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4903</td>\n",
       "      <td>Бывший защитник сборной России Дарюс Каспарайт...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4904</td>\n",
       "      <td>Полузащитник ЦСКА Зоран Тошич после победы над...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4905</td>\n",
       "      <td>Вратарь «Барселоны» Виктор Вальдес пропустит м...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4906</td>\n",
       "      <td>Россиянка Анастасия Павлюченкова стала победит...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4907</td>\n",
       "      <td>Нападающий «Анжи» Лассина Траоре вернулся к тр...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                              title\n",
       "0        6  Заместитель председателяnправительства РФnСерг...\n",
       "1     4896  Матч 1/16 финала Кубка России по футболу был п...\n",
       "2     4897  Форвард «Авангарда» Томаш Заборский прокоммент...\n",
       "3     4898  Главный тренер «Кубани» Юрий Красножан прокомм...\n",
       "4     4899  Решением попечительского совета владивостокско...\n",
       "5     4900  И.о. главного тренера Вячеслав Буцаев прокомме...\n",
       "6     4901  Запорожский «Металлург» дома потерпел разгромн...\n",
       "7     4902  Сборная США одержала победу над Австрией со сч...\n",
       "8     4903  Бывший защитник сборной России Дарюс Каспарайт...\n",
       "9     4904  Полузащитник ЦСКА Зоран Тошич после победы над...\n",
       "10    4905  Вратарь «Барселоны» Виктор Вальдес пропустит м...\n",
       "11    4906  Россиянка Анастасия Павлюченкова стала победит...\n",
       "12    4907  Нападающий «Анжи» Лассина Траоре вернулся к тр..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# смотрим данные по статьям\n",
    "\n",
    "news = pd.read_csv(\"articles.csv\")\n",
    "print(news.shape)\n",
    "news.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>[293672, 293328, 293001, 293622, 293126, 1852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>[3405, 1739, 2972, 1158, 1599, 322665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>[1845, 2009, 2356, 1424, 2939, 323389]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u101138</td>\n",
       "      <td>[5933, 6186, 5055, 6977, 5206, 488389]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u108248</td>\n",
       "      <td>[707, 1144, 2532, 2928, 3133, 324592]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>u106662</td>\n",
       "      <td>[323868, 323426, 324267, 322426, 324104, 1550]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>u105949</td>\n",
       "      <td>[293138, 294471, 295012, 294736, 293949, 3544]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>u102457</td>\n",
       "      <td>[6928, 5009, 6940, 7629, 7644, 512736]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>u104124</td>\n",
       "      <td>[322838, 324699, 322991, 322120, 324327, 472331]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>u101386</td>\n",
       "      <td>[7827, 6427, 7394, 7151, 6335, 487254]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>u104519</td>\n",
       "      <td>[322147, 322974, 322922, 74, 322665, 472624]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>u101068</td>\n",
       "      <td>[6163, 7517, 6678, 7482, 5124, 511479]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>u103480</td>\n",
       "      <td>[323041, 322782, 322341, 323296, 324213, 473474]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid                                          articles\n",
       "0   u105138    [293672, 293328, 293001, 293622, 293126, 1852]\n",
       "1   u108690            [3405, 1739, 2972, 1158, 1599, 322665]\n",
       "2   u108339            [1845, 2009, 2356, 1424, 2939, 323389]\n",
       "3   u101138            [5933, 6186, 5055, 6977, 5206, 488389]\n",
       "4   u108248             [707, 1144, 2532, 2928, 3133, 324592]\n",
       "5   u106662    [323868, 323426, 324267, 322426, 324104, 1550]\n",
       "6   u105949    [293138, 294471, 295012, 294736, 293949, 3544]\n",
       "7   u102457            [6928, 5009, 6940, 7629, 7644, 512736]\n",
       "8   u104124  [322838, 324699, 322991, 322120, 324327, 472331]\n",
       "9   u101386            [7827, 6427, 7394, 7151, 6335, 487254]\n",
       "10  u104519      [322147, 322974, 322922, 74, 322665, 472624]\n",
       "11  u101068            [6163, 7517, 6678, 7482, 5124, 511479]\n",
       "12  u103480  [323041, 322782, 322341, 323296, 324213, 473474]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# данные какой пользователь какими статьями интересовался\n",
    "\n",
    "users = pd.read_csv(\"users_articles.csv\")\n",
    "users.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем слова не несущие смысла - предлоги всякие и т.д.\n",
    "\n",
    "stopword_ru = stopwords.words('russian')\n",
    "len(stopword_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopword_ru] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-7ee348d9b386>:15: FutureWarning: Possible nested set at position 39\n",
      "  text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# чистим текст \n",
    "news['title'] = news['title'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        заместитель председателяnправительства рфnсерг...\n",
       "1        матч  финала кубка россии по футболу был приос...\n",
       "2        форвард авангарда томаш заборский прокомментир...\n",
       "3        главный тренер кубани юрий красножан прокоммен...\n",
       "4        решением попечительского совета владивостокско...\n",
       "                               ...                        \n",
       "26995    ученые токийского университета морских наук и ...\n",
       "26996    главой кафедры отечественной истории xx века и...\n",
       "26997    американские ученые уточнили возраст расположе...\n",
       "26998    за последние  лет тропический углеродный цикл ...\n",
       "26999    у живших примерно  тыс лет назад на территории...\n",
       "Name: title, Length: 27000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# леммитизация текста - перевод слов в именительный падеж, единственное число или типа того...\n",
    "news['title'] = news['title'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сформируем список наших текстов, разбив еще и на пробелы\n",
    "texts = [t for t in news['title'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts\n",
    "\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'выбросить'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_dictionary[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141492"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, \n",
    "               num_topics=25, # количество кластеров, т.е. тем, на которые мы хотим разбить наши тексты \n",
    "               id2word=common_dictionary)#, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['форвард', 'авангард', 'томаш', 'заборский', 'прокомментировать', 'игра', 'свой', 'команда', 'матч', 'чемпионат', 'кхл', 'против', 'атланта', 'nnnn', 'провести', 'плохой', 'матч', 'нижний', 'новгород', 'против', 'торпедо', 'настраиваться', 'первый', 'минута', 'включиться', 'работа', 'сказать', 'заборский', 'получиться', 'забросить', 'быстрый', 'гол', 'задать', 'хороший', 'темп', 'поединок', 'мочь', 'играть', 'ещё', 'хороший', 'сторона', 'пять', 'очко', 'выезд', 'девять', 'это', 'хороший']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 0.076924816),\n",
       " (4, 0.19794227),\n",
       " (6, 0.15132573),\n",
       " (12, 0.18508969),\n",
       " (18, 0.13451609),\n",
       " (22, 0.23724996)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [t for t in news['title'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[2]\n",
    "print(other_texts[2])\n",
    "lda[unseen_doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0: препарат убийство следователь расследование площадь ск лечение\n",
      "topic_1: год россия который страна это свой nn\n",
      "topic_2: фестиваль экипаж век белоруссия китайский флот пилотировать\n",
      "topic_3: продукция чёрный сутки сообщество штраф суд обязательство\n",
      "topic_4: минск тур макаров молдавия донбасс норматив обернуться\n",
      "topic_5: агентство высота год звезда необычный сообщать след\n",
      "topic_6: это который мочь год система россия рынок\n",
      "topic_7: обнаружить тело результат мозг это человек взрыв\n",
      "topic_8: это который год сша украина эксперт американский\n",
      "topic_9: фонд власть закон правительство орган человек организация\n",
      "topic_10: the космос доля годовой атмосферный брюссель разбираться\n",
      "topic_11: технология долг это отряд век свидетель индия\n",
      "topic_12: год это который человек исследование ребёнок время\n",
      "topic_13: год тыс цена рост млн снижение доход\n",
      "topic_14: год который банк это компания суд также\n",
      "topic_15: греция рейтинг подсчитать австралийский болгария су инвестировать\n",
      "topic_16: конкурс супруг ночной жена гора рисунок корь\n",
      "topic_17: ракета статья дело задержать депутат сотрудник запуск\n",
      "topic_18: газ остров который британский это участок запуск\n",
      "topic_19: год рубль млрд млн доллар бюджет напомнить\n",
      "topic_20: восток оборудование дональд край согласование фаза ресурс\n",
      "topic_21: погода дождь умирать платёжный ухо спасти грунт\n",
      "topic_22: путин смерть эксперимент президент рак владимир пресссекретарить\n",
      "topic_23: земля журнал планета топливо городской треть виза\n",
      "topic_24: квартира дом производитель северный физика одежда южный\n"
     ]
    }
   ],
   "source": [
    "x=lda.show_topics(num_topics=25, num_words=7,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функиця представляет векторное представление новости\n",
    "def get_lda_vector(text):\n",
    "      \n",
    "    unseen_doc = common_dictionary.doc2bow(text)\n",
    "    lda_tuple = lda[unseen_doc]\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(25):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "# п.4 добавил функцию, которая берет только слова поTfidf с весом больше 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция с добавлением TfIdf \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_lda_vector_idf(text):\n",
    "    \n",
    "    # !!! ОБАВЛЕНИЯ ТУТ !!!\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit_transform(text)\n",
    "    new_text = {}\n",
    "    for key, value in vectorizer.vocabulary_.items():\n",
    "        if value > 80:\n",
    "            new_text[key] = key  \n",
    "    \n",
    "    unseen_doc = common_dictionary.doc2bow(new_text)\n",
    "    lda_tuple = lda[unseen_doc]\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(25):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "      <th>topic_20</th>\n",
       "      <th>topic_21</th>\n",
       "      <th>topic_22</th>\n",
       "      <th>topic_23</th>\n",
       "      <th>topic_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071803</td>\n",
       "      <td>0.224575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.681612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316961</td>\n",
       "      <td>0.110048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  topic_0  topic_1   topic_2   topic_3   topic_4  topic_5   topic_6  \\\n",
       "0       6      0.0      0.0  0.046628  0.000000  0.025532      0.0  0.000000   \n",
       "1    4896      0.0      0.0  0.000000  0.071803  0.224575      0.0  0.000000   \n",
       "2    4897      0.0      0.0  0.076918  0.000000  0.197938      0.0  0.151241   \n",
       "3    4898      0.0      0.0  0.000000  0.000000  0.118391      0.0  0.316961   \n",
       "4    4899      0.0      0.0  0.000000  0.000000  0.087253      0.0  0.000000   \n",
       "\n",
       "    topic_7  topic_8  ...  topic_15  topic_16  topic_17  topic_18  topic_19  \\\n",
       "0  0.000000      0.0  ...       0.0       0.0  0.113529  0.000000       0.0   \n",
       "1  0.681612      0.0  ...       0.0       0.0  0.000000  0.000000       0.0   \n",
       "2  0.000000      0.0  ...       0.0       0.0  0.000000  0.134581       0.0   \n",
       "3  0.110048      0.0  ...       0.0       0.0  0.000000  0.000000       0.0   \n",
       "4  0.000000      0.0  ...       0.0       0.0  0.353743  0.000000       0.0   \n",
       "\n",
       "   topic_20  topic_21  topic_22  topic_23  topic_24  \n",
       "0  0.000000       0.0  0.000000       0.0  0.000000  \n",
       "1  0.000000       0.0  0.000000       0.0  0.000000  \n",
       "2  0.000000       0.0  0.237265       0.0  0.000000  \n",
       "3  0.142180       0.0  0.000000       0.0  0.000000  \n",
       "4  0.051422       0.0  0.000000       0.0  0.038943  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix = pd.DataFrame([get_lda_vector(text) for text in news['title'].values])\n",
    "# topic_matrix = pd.DataFrame([get_lda_vector_idf(text) for text in news['title'].values])\n",
    "topic_matrix.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "topic_matrix['doc_id'] = news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id']+['topic_{}'.format(i) for i in range(25)]]\n",
    "topic_matrix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переходим к работе с пользователями\n",
    "\n",
    "# users.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[['topic_{}'.format(i) for i in range(25)]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# на примере одного документа смотрим какие темы в нем затронуты\n",
    "\n",
    "# doc_dict[293622]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_articles_list = users['articles'].iloc[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[323329, 321961, 324743, 323186, 324632, 474690]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# на примере пользователя выбираем статьи, которыми он интересовался\n",
    "\n",
    "user_articles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Датасет готов - можно попробовать обучить модель. Загрузим нашу разметку\n",
    "\n",
    "target = pd.read_csv(\"users_churn.csv\")\n",
    "# target.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ 3\n",
    "\n",
    "# ОСНОВНАЯ ДОМАШКА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding_mean(user_articles_list):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector = np.mean(user_vector, 0)\n",
    "    return user_vector\n",
    "\n",
    "def get_user_embedding_median(user_articles_list):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector = np.median(user_vector, 0)\n",
    "    return user_vector\n",
    "\n",
    "def get_user_embedding_max(user_articles_list):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector = np.max(user_vector, 0)\n",
    "    return user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrix_(user_embeddings):\n",
    "    user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "    user_embeddings['uid'] = users['uid'].values\n",
    "    user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "    # user_embeddings.head(3)\n",
    "\n",
    "    X = pd.merge(user_embeddings, target, 'left')\n",
    "\n",
    "    #разделим данные на train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]], \n",
    "                                                        X['churn'], random_state=0)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    #обучим \n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    #наши прогнозы для тестовой выборки\n",
    "    preds = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # считаем Precision, Recall, F_score\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    # locate the index of the largest f score\n",
    "    ix = np.argmax(fscore)\n",
    "    print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f, roc_auc_score=%.3f' % (thresholds[ix], \n",
    "                                                                            fscore[ix],\n",
    "                                                                            precision[ix],\n",
    "                                                                            recall[ix],\n",
    "                                                                            roc_auc_score(y_test, preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n",
      "Best Threshold=0.258392, F-Score=0.696, Precision=0.691, Recall=0.702, roc_auc_score=0.951\n",
      "median:\n",
      "Best Threshold=0.247426, F-Score=0.706, Precision=0.639, Recall=0.788, roc_auc_score=0.955\n",
      "max:\n",
      "Best Threshold=0.373306, F-Score=0.777, Precision=0.801, Recall=0.755, roc_auc_score=0.968\n"
     ]
    }
   ],
   "source": [
    "# Теперь получим эмбединги для всех пользователей и проверим их качество на конкретной downstream-задаче\n",
    "\n",
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding_mean(x), 1)])\n",
    "print('mean:')\n",
    "metrix_(user_embeddings)\n",
    "\n",
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding_median(x), 1)])\n",
    "print('median:')\n",
    "metrix_(user_embeddings)\n",
    "\n",
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding_max(x), 1)])\n",
    "print('max:')\n",
    "metrix_(user_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь смотрю, какие метрики, если использовать TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n",
      "Best Threshold=0.360473, F-Score=0.879, Precision=0.840, Recall=0.922, roc_auc_score=0.990\n",
      "median:\n",
      "Best Threshold=0.448672, F-Score=0.964, Precision=0.952, Recall=0.976, roc_auc_score=0.998\n",
      "max:\n",
      "Best Threshold=0.149007, F-Score=0.544, Precision=0.430, Recall=0.739, roc_auc_score=0.903\n"
     ]
    }
   ],
   "source": [
    "# topic_matrix = pd.DataFrame([get_lda_vector(text) for text in news['title'].values])\n",
    "topic_matrix = pd.DataFrame([get_lda_vector_idf(text) for text in news['title'].values])\n",
    "topic_matrix.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "topic_matrix['doc_id'] = news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id']+['topic_{}'.format(i) for i in range(25)]]\n",
    "# topic_matrix.head(5)\n",
    "\n",
    "doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[['topic_{}'.format(i) for i in range(25)]].values))\n",
    "\n",
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding_mean(x), 1)])\n",
    "print('mean:')\n",
    "metrix_(user_embeddings)\n",
    "\n",
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding_median(x), 1)])\n",
    "print('median:')\n",
    "metrix_(user_embeddings)\n",
    "\n",
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding_max(x), 1)])\n",
    "print('max:')\n",
    "metrix_(user_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод:\n",
    "\n",
    "1. Без TfIdf лучшие метрики получаются если брать max(). Думаю это оттого что при max - это тема, которая больше всего интересовала данного пользователя, т.е. мы опираем на лучшие данные.\n",
    "\n",
    "2. При использовании TfIdf метрики на mean() и median() хорошо так возрастают. А вот при max() наоборот снижаются. Думаю оттого, что max() при использовании TfIdf не настроящий...\n",
    "\n",
    "3. В общем использвание TfIdf сильно улучшает результаты! Самые лучшие метрики при TfIdf + median(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
